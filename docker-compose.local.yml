version: '3.8'

services:
  # Ollama service for local LLM hosting
  ollama:
    image: ollama/ollama:latest
    container_name: fintelligence-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./data/models:/models  # Optional: local model storage
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    networks:
      - fintelligence-local
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ChromaDB for vector storage
  chromadb:
    image: chromadb/chroma:latest
    container_name: fintelligence-chromadb-local
    ports:
      - "8100:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_PORT=8000
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["*"]
    networks:
      - fintelligence-local
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis for caching (optional in local mode)
  redis:
    image: redis:7-alpine
    container_name: fintelligence-redis-local
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./config/redis/redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - fintelligence-local
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # FintelligenceAI main application
  fintelligence-ai:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: fintelligence-ai-local
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./config:/app/config
      - ./.env:/app/.env
    environment:
      # Local mode configuration
      - DSPY_LOCAL_MODE=true
      - DSPY_MODEL_PROVIDER=ollama

      # Ollama configuration
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
      - OLLAMA_MODEL=llama3.2
      - OLLAMA_EMBEDDING_MODEL=nomic-embed-text
      - OLLAMA_TEMPERATURE=0.1
      - OLLAMA_MAX_TOKENS=4096
      - OLLAMA_TIMEOUT=300
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_VERIFY_SSL=false

      # ChromaDB configuration
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - CHROMA_PERSIST_DIRECTORY=/app/data/chroma

      # Redis configuration
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0

      # Application configuration
      - APP_ENVIRONMENT=local
      - APP_DEBUG=true
      - APP_LOG_LEVEL=INFO

      # API configuration
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - API_CORS_ORIGINS=["http://localhost:3000", "http://localhost:8080"]

      # Disable external services (optional)
      - LANGCHAIN_TRACING_V2=false
      - SENTRY_DSN=
    depends_on:
      ollama:
        condition: service_healthy
      chromadb:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - fintelligence-local
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Model initialization service
  model-init:
    image: curlimages/curl:latest
    container_name: fintelligence-model-init
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - fintelligence-local
    command: >
      sh -c "
        echo 'Pulling required models...';
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"llama3.2\"}' -H 'Content-Type: application/json';
        sleep 10;
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"nomic-embed-text\"}' -H 'Content-Type: application/json';
        sleep 10;
        echo 'Models pulled successfully';
      "
    restart: "no"

networks:
  fintelligence-local:
    driver: bridge

volumes:
  ollama_data:
    driver: local
  chroma_data:
    driver: local
  redis_data:
    driver: local
